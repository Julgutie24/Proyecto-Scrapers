# Proyecto-Scrapers


Este proyecto utiliza **[Playwright](https://playwright.dev/python/)** para automatizar la navegaci√≥n en p√°ginas web con el objetivo de extraer informaci√≥n √∫til de manera estructurada. Est√° dividido en dos partes complementarias:

- **Parte 1 ‚Äì Scraper Retail (sincr√≥nico)**: Automatiza la extracci√≥n de productos desde tiendas online como **MercadoLibre** y **√âxito**.
- **Parte 2 ‚Äì Scraper News (asincr√≥nico)**: Extrae informaci√≥n estructurada desde paginas de noticias.
- **Parte 3 ‚Äì Scraper Wiki (sincr√≥nico)**: Extrae informaci√≥n estructurada desde art√≠culos de una Wiki.

Esto permite comparar dos enfoques distintos de programaci√≥n (sincr√≥nico y asincr√≥nico), ambos usando Playwright con Python.

---

## ü§ñ ¬øQu√© es Playwright?

Playwright es una herramienta poderosa que permite controlar navegadores como **Chromium, Firefox y WebKit** desde c√≥digo. Sirve para:

- Automatizar pruebas web (como Selenium)
- Hacer scraping de sitios din√°micos (que cargan con JavaScript)
- Interactuar con elementos como botones, formularios, scrolls, etc.

Es compatible con m√∫ltiples lenguajes como **Python**, **JavaScript** y **C#**.  
En este proyecto usamos **Playwright para Python**.

---

## ¬øQu√© es programaci√≥n Sincr√≥nica y Asincr√≥nica?

Playwright puede ejecutarse de dos maneras diferentes en Python:

| Tipo        | Descripci√≥n                                                                 |
|-------------|------------------------------------------------------------------------------|
|  **Sincr√≥nica** | Ejecuta cada instrucci√≥n **una tras otra**, esperando a que termine para pasar a la siguiente. M√°s f√°cil de entender y depurar. Ideal para proyectos simples. |
|  **Asincr√≥nica** | Permite realizar m√∫ltiples tareas al mismo tiempo usando `async/await`. Ideal para scraping en paralelo o cuando se necesitan muchas esperas. |

---

### Ejemplo de uso Sincr√≥nico (como en el scraper retail):

```python
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()
    page.goto("https://example.com")
    print(page.title())
    browser.close()
```
### Ejemplo de uso Asincr√≥nico (como en el scraper wiki):

```python
from playwright.async_api import async_playwright
import asyncio

async def run():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto("https://example.com")
        print(await page.title())
        await browser.close()

asyncio.run(run())
```
# Parte 1 - Scraper Retail
## Bibliotecas utilizadas
```python
from playwright.sync_api import sync_playwright
import csv
import time
import random
from datetime import datetime
import os
import json
from abc import ABC, abstractmethod
```
* **csv:** Generar archivos CSV (valores separados por comas) para exportar datos.
```python
with open("reporte.csv", "w") as file:
    writer = csv.DictWriter(file, fieldnames=["producto", "precio"])
    writer.writeheader()
    writer.writerow({"producto": "Zapatos", "precio": "$100"})
#producto,precio
#Zapatos,$100
```
*  **time:** Controlar tiempos de espera entre acciones.
*  **random:** Generar aleatoriedad en tiempos de espera y acciones.
*   **datetime:**
```python
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
# Genera: "20240521_143022" (a√±o, mes, d√≠a, hora, minuto, segundo)
```
se usa para nombres de archivos √∫nicos (reporte_20240521.csv) y registro de cu√°ndo se extrajeron los datos.
* **os:** Interactuar con el sistema operativo (archivos, rutas).
```python
os.makedirs("reportes", exist_ok=True)  # Crea carpeta si no existe
os.path.join("reportes", "data.csv")    # Genera rutas multiplataforma (Windows/Linux/Mac)
```
* **json:** Exportar datos en formato JSON (intercambio de datos con APIs o apps web).
```python
with open("data.json", "w") as file:
    json.dump(productos, file, indent=2)  # indent=2 para formato legible
#ejemplo de uso
#"producto": "iPhone 15",
#"precio": "$5.000.000"
```
```mermaid
flowchart TD
    A[playwright] -->|Controla| B[Navegador]
    C[csv/json] -->|Exporta| D[Reportes]
    E[time/random] -->|Simula| F[Comportamiento humano]
    G[abc] -->|Define| H[Plantilla para scrapers]
```
## Clase Base abstracta: BaseRetailScraper
Prop√≥sito: Proporcionar una estructura com√∫n para scrapers de e-commerce con funcionalidades compartidas.
### 1.Inicializacion
```python
def __init__(self, site_name, base_url=None):
    self.site_name = site_name
    self.base_url = base_url
    self.user_agent = "Mozilla/5.0 (Windows NT 10.0...) AppleWebKit/537.36"
    self.viewport = {"width": 1366, "height": 768}
    self.report_dir = "reportes_retail"
    # Selectores (deben definirse en clases hijas)
    self.search_input_selector = None
    self.product_container_selector = None
    # ... (otros selectores)
```
#### Responsabilidades
* Configuraci√≥n b√°sica del navegador (User-Agent, resoluci√≥n)
* Definici√≥n de selectores HTML (deben ser sobrescritos)
* Creaci√≥n de carpeta para reportes (reportes_retail)
### 2. M√©todo Abstracto Obligatorio
```python
@abstractmethod
def scrape(self, producto: str, paginas: int = 1):
    """M√©todo principal a implementar por cada scraper concreto"""
    pass
```
#### ¬øpor qu√©?
* Estructura HTML √∫nica
* Flujos de navegaci√≥n diferentes
### 3. Navegaci√≥n y Browser
#### Configuraci√≥n del Navegador
```python
def _setup_browser(self):
    browser = sync_playwright().start().chromium.launch(
        headless=False,
        args=['--disable-blink-features=AutomationControlled']
    )
```
##### Caracteristicas Clave
* Pantalla visible
* Evitar ser detectados
* Configuraacion regional
#### Busqueda de productos
```python
def _realizar_busqueda(self, page, producto):
    search_input = page.wait_for_selector(self.search_input_selector)
    search_input.fill(producto)
```
* Espera campo de busqueda
* Busca el producto
### 4. Manejo de Datos
#### Extracci√≥n Estructurada
```python
def _extraer_datos_producto(self, item):
    return {
        "producto": nombre,
        "precio_actual": f"${precio} COP",
        "descuento": descuento,
        # ... (otros campos)
    }
```
* C√°lculo de descuentos si no estan visible
* url al producto
#### Exportaci√≥n de Resultados
```python
def _guardar_resultados(self, productos: list, producto: str):
    # CSV
    with open(f"{nombre_archivo}.csv", "w") as file:
        writer = csv.DictWriter(...)
    # JSON
    with open(f"{nombre_archivo}.json", "w") as file:
        json.dump(...)
```
* CSV: Para an√°lisis en Excel/Google Sheets
* JSON: Para integraciones con otras aplicaciones
### 5. Utilidades Avanzadas
#### Manejo de errores
```python
def _ir_a_siguiente_pagina(self, page):
    try:
        next_btn = page.locator(self.next_page_selector)
        next_btn.click(force=True)
    except Exception as e:
        print(f"Error: {str(e)}")
        return False
```
* Verifica existencia de bot√≥n
* Fuerza clics si es necesario
* Registra errores sin detener la ejecuci√≥n
#### Anti-Detecci√≥n
```python
def _esperar_carga(self, min=2, max=4):
    delay = random.uniform(min, max)
    time.sleep(delay)
```
* Esperas aleatorias entre acciones
* Simula velocidad humana de navegaci√≥n
## ¬øCom√≥ funciona?
```mermaid
flowchart TD
    A[Inicio] --> B[setup_browser]
    B --> C[realizar_busqueda]
    C --> D[extraer_datos_producto]
    D --> E{¬øM√°s p√°ginas?}
    E -->|S√≠| F[ir_a_siguiente_pagina]
    E -->|No| G[guardar_resultados]
```
##Ejemplo de scraper - Mercado Libre
```python
class MercadoLibreScraper(BaseRetailScraper):
    """Scraper especializado para Mercado Libre Colombia"""
    
    def __init__(self):
        super().__init__("MercadoLibre", "https://www.mercadolibre.com.co")
        
        # Definici√≥n de selectores espec√≠ficos
        self.search_input_selector = "input.nav-search-input"
        self.search_button_selector = None  # Se usa Enter
        self.product_container_selector = "div.ui-search-result"
        self.product_name_selector = "h2.ui-search-item__title"
        self.product_price_selector = "span.andes-money-amount__fraction"
        self.product_original_price_selector = "s.andes-money-amount__fraction"
        self.product_link_selector = "a.ui-search-link"
        self.product_discount_selector = "span.ui-search-price__discount"
        self.next_page_selector = "li.andes-pagination__button--next a"
        self.cookie_accept_selector = "button.cookie-consent-banner-opt-out__action"
```
* Selectores actualizados
* Busqueda por enter
### Metodo scrape(donde esta la magia)
#### 1. Inicializaci√≥n del Browser
```python
browser, page = self._setup_browser()
```
* Lanza un navegador Chromium
* Crea una nueva pesta√±a (page) lista para navegar.
#### 2. Navegaci√≥n Inicial
```python
page.goto(self.base_url, timeout=60000)
self._manejar_cookies(page)
```
* Carga la p√°gina principal de Mercado Libre.
* Intenta cerrar el popup de cookies (si existe).
#### 3. B√∫squeda M√°gica
```python
self._realizar_busqueda(page, "iPhone 15")
```
* Localiza el input de b√∫squeda usando self.search_input_selector.
* Simula escritura humana con .fill() + _esperar_carga().
#### 4. Extracci√≥n de Productos
```python
items = page.query_selector_all(self.product_container_selector)
for item in items:
    producto_data = self._extraer_datos_producto(item)
```
* query_selector_all(): Captura todos los contenedores de productos (ej: "<div class="ui-search-result"">).
* _extraer_datos_producto() por cada √≠tem:
#### Paginaci√≥n Autom√°tica
```python
if pagina_actual < paginas and not self._ir_a_siguiente_pagina(page):
    break
```
* Usa self.next_page_selector (li.andes-pagination__button--next a).
* Hover + Click forzado: Simula comportamiento humano real.
## RetailScraper(orquestador)
```python
  def scrape(self, producto: str, sitio: str, paginas: int = 1):
        sitio = sitio.lower()  # Obtiene MercadoLibreScraper o ExitoScraper
        if sitio in self.scrapers:
            return self.scrapers[sitio].scrape(producto, paginas) # Delega el trabajo al scraper espec√≠fico
```
# Parte 2 - Scraper de Noticias (NewScraper)

### Bibliotecas utilizadas

```python
import asyncio
from playwright.async_api import async_playwright
from herramientas import process_text
```
```mermaid
flowchart LR
    A[asyncio] -->|Gestiona| B[Concurrencia de tareas]
    C[playwright.async_api] -->|Controla| D[Navegador en modo headless/headful]
    E[herramientas] -->|Procesa| F[Extracci√≥n de texto y res√∫menes]
```
## Clase `NewScraper`

### 1. Configuraci√≥n Inicial
```python
  class NewScraper:
    TIMEOUT = 30000  # 30 segundos
    MAX_RESULTS = 3  # M√°ximo de noticias por sitio
    HEADLESS = False  # Modo visible
```
- Define constantes clave para el scraper de noticias:
  - `TIMEOUT`: Tiempo m√°ximo de espera para cargar una p√°gina (30‚ÄØs).
  - `MAX_RESULTS`: L√≠mite de noticias extra√≠das por sitio.
  - `HEADLESS`: Indica si el navegador se ejecuta en modo oculto (`True`) o visible (`False`).

### 2. Funcionalidad Principal
#### scrappers especificos por sitio:
```python
  async def eltiempo_scraper(self, page, keyword):
    # Configuraci√≥n √∫nica para El Tiempo
    search_url = f"https://www.eltiempo.com/buscar?q={self.normalize_keyword(keyword, 'eltiempo')}"
    await page.goto(search_url, timeout=self.TIMEOUT)
    items = await page.locator("h3.c-article__title a").all()
```
- Realiza una b√∫squeda en el sitio web de El Tiempo con la palabra clave dada.
- Construye la URL de b√∫squeda reemplazando espacios por `+`.
- Navega a la p√°gina de resultados y localiza los enlaces de titulares (`<h3>` con clase `c-article__title`).

#### Flujo de extracci√≥n:
```python
  async def scrape_eltiempo_article(self, page):
    title = await page.locator("h1").first.inner_text()
    paragraphs = await page.locator("div.paragraph").all_text_contents()
    full_text = "\n\n".join(paragraphs)
    process_text(page.url, "Noticia", full_text)
```
- Extrae el t√≠tulo principal del art√≠culo desde la etiqueta `<h1>`.
- Obtiene todo el contenido del cuerpo usando los p√°rrafos dentro de `div.paragraph`.
- Une los p√°rrafos en un solo texto y lo env√≠a a `process_text` junto con la URL y la etiqueta "Noticia".

### 3. Metodos Clave
```python
  async def scraper(self, keyword):
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=self.HEADLESS)
        page = await browser.new_page()
        await self.eltiempo_scraper(page, keyword)
        # [...] otros scrapers
        await browser.close()
```
- Inicia una instancia de navegador usando `async_playwright` en modo asincr√≥nico.
- Crea una nueva pesta√±a y ejecuta el scraper espec√≠fico para El Tiempo con la palabra clave.
- Al finalizar, cierra el navegador limpiamente. Puede ampliarse para incluir otros sitios.

### 4. Ejemplo de uso
```python
  async def run():
    scraper = NewScraper()
    await scraper.scraper("elecciones 2023")
    
asyncio.run(run())
```
- Crea una instancia del scraper de noticias (`NewScraper`).
- Ejecuta el m√©todo principal `scraper` con la palabra clave `"elecciones 2023"`.
- Usa `asyncio.run()` para lanzar la rutina asincr√≥nica desde un contexto sincr√≥nico.

## Parte 3 - WikiScraper
### Bibliotecas utilizadas
```python
  from playwright.sync_api import sync_playwright
from herramientas import process_text
```
### Clase WikiScraper
### 1. Estructura Principal
```python
  class WikiScraper:
    def scraper(self, urls):
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=False)
            page = browser.new_page()
            page.goto(url)
```
- Abre una sesi√≥n de navegador en modo sincr√≥nico usando `sync_playwright`.
- Lanza un navegador Chromium visible (`headless=False`) y abre una nueva pesta√±a.
- Navega a la URL proporcionada para comenzar el proceso de scraping.
### 2. Proceso de Extracci√≥n
#### Detecci√≥n de secciones
```python
  sections = page.locator("h2").all_inner_texts()
banned = {"Contenidos", "Referencias", "Enlaces externos"}
filtered = [s for s in sections if s not in banned]
```
- Extrae todos los t√≠tulos de segundo nivel (`<h2>`) de la p√°gina.
- Define un conjunto de secciones que deben excluirse (`banned`).
- Filtra las secciones eliminando aquellas que est√©n en la lista prohibida.
#### Extracci√≥n de contenido
```python
  if selected_section == "Introducci√≥n":
    paragraphs = page.locator("p")
    for p in paragraphs.all():
        if p.locator("xpath=preceding-sibling::h2").count() > 0:
            break
        full_text += p.inner_text() + "\n\n"
```
- Si la secci√≥n seleccionada es "Introducci√≥n", se recorren los p√°rrafos (`<p>`) iniciales de la p√°gina.
- Se detiene la lectura cuando aparece el primer `<h2>`, indicando el fin de la introducci√≥n.
- Se acumula el texto de cada p√°rrafo relevante en la variable `full_text`, separado por saltos de l√≠nea.
### 3. Ejemplo de Output
```python
  process_text(
    url="https://es.wikipedia.org/wiki/Python",
    seccion="Historia", 
    texto="Guido van Rossum cre√≥ Python...",
    modo="resumen_IA"
)
```
``` mermaid
flowchart TD
    Retail[RetailScraper] -->|Sincr√≥nico| CSV[(CSV/JSON)]
    News[NewScraper] -->|As√≠ncrono| IA[(Res√∫menes IA)]
    Wiki[WikiScraper] -->|Estructurado| Secciones[(Secciones Wiki)]
```
